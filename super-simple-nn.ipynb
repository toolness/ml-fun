{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atul's super simple neural net\n",
    "\n",
    "_October 8, 2017_\n",
    "\n",
    "This workbook contains an implementation of a ridiculously simple neural net with one hidden layer that has two units, and one unit in the output layer. The sigmoid activation function is used in both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define the sigmoid function and its gradient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.power(np.e, -x))\n",
    "\n",
    "# Sanity check.\n",
    "assert sigmoid(0) == 0.5\n",
    "assert_almost_equal(sigmoid(-100), 0)\n",
    "assert_almost_equal(sigmoid(100), 1)\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Sanity check.\n",
    "assert_almost_equal(sigmoid_gradient(-100), 0)\n",
    "assert_almost_equal(sigmoid_gradient(100), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function that runs forward propagation on a neural net, given the weights and biases for both layers.\n",
    "\n",
    "In implementing this, I found Coursera's deeplearning.ai course on [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning) useful; all notation is generally taken from that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_nn(w1, b1, w2, b2, x):\n",
    "    z1 = np.dot(w1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    return (z1, a1, z2, a2)\n",
    "\n",
    "def predict_nn(w1, b1, w2, b2, x):\n",
    "    return forward_prop_nn(w1, b1, w2, b2, x)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will manually create a neural net to run the XNOR boolean operation. XNOR is just the negation of XOR, i.e. it will be true if _both_ its inputs are true or false, and false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnor_w_1 = np.array([\n",
    "    [-20, -20],          # Weights for \"(NOT x[0]) AND (NOT x[1])\" \n",
    "    [ 20,  20],          # Weights for \"x[0] AND x[1]\"\n",
    "]) * 10\n",
    "xnor_b_1 = np.array([\n",
    "    [ 10],               # Bias for \"(NOT x[0]) AND (NOT x[1])\"\n",
    "    [-30],               # Bias for \"x[0] AND x[1]\"\n",
    "]) * 10\n",
    "xnor_w_2 = np.array([\n",
    "    [ 20,  20],          # Weights for \"x[0] OR x[1]\"\n",
    "]) * 10\n",
    "xnor_b_2 = np.array([\n",
    "    [-10],               # Bias for \"x[0] OR x[1]\"\n",
    "]) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make sure our manually-constructed NN matches our intuitive expectations of XNOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a \"truth table\" for our XNOR function. We'll use this to make sure our NN\n",
    "# works, and we'll also use it later as training data.\n",
    "boolean_xnor_truth_table = [\n",
    "    # x[0]   x[1]    y\n",
    "    [(True , True ), True ],\n",
    "    [(False, False), True ],\n",
    "    [(False, True ), False],\n",
    "    [(True , False), False]\n",
    "]\n",
    "\n",
    "# This is a numpy-friendly version of our truth table, where each item is\n",
    "# a tuple consisting of a 2x1 array representing the input (x) and a 1x1\n",
    "# array representing the output (y).\n",
    "xnor_truth_table = [\n",
    "    (np.array(x, dtype=float).reshape(2, 1),\n",
    "     np.array([[y]], dtype=float))\n",
    "    for (x, y) in boolean_xnor_truth_table\n",
    "]\n",
    "\n",
    "# Test our NN to make sure everything works.\n",
    "for x, y in xnor_truth_table:\n",
    "    assert_almost_equal(predict_nn(xnor_w_1, xnor_b_1, xnor_w_2, xnor_b_2, x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function to train a neural net!\n",
    "\n",
    "This is intentionally un-vectorized because I wanted to make sure I understood the algorithm before dealing with vectorization. Thus the `examples` parameter is just a Python list of tuples containing a 2x1 numpy array and an expected 1x1 output.\n",
    "\n",
    "The Coursera class' [Backpropagation intuition](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional) lecture was particularly helpful in understanding the math behind this. I supplemented my understanding with [Khan Academy's AP Calculus AB](https://www.khanacademy.org/math/ap-calculus-ab) when needed because I am very rusty at Calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(predicted_y, y):\n",
    "    return -y * np.log(predicted_y) - (1 - y) * np.log(1 - predicted_y)\n",
    "\n",
    "def train_nn(examples, iterations, learning_rate, check_gradient=None, print_cost=True):\n",
    "    m = len(examples)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # Initialize our weights and biases. Note that the weights need to\n",
    "    # be randomly initialized so we can break symmetry.\n",
    "    w1 = np.random.rand(2, 2)\n",
    "    b1 = np.zeros([2, 1])\n",
    "    w2 = np.random.rand(1, 2)\n",
    "    b2 = np.zeros([1, 1])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        dw1 = np.zeros([2, 2])\n",
    "        db1 = np.zeros([2, 1])\n",
    "        dw2 = np.zeros([1, 2])\n",
    "        db2 = np.zeros([1, 1])\n",
    "        cost = np.zeros([1, 1])\n",
    "        for x, y in examples:\n",
    "            # Forward propagation.\n",
    "            z1, a1, z2, a2 = forward_prop_nn(w1, b1, w2, b2, x)\n",
    "\n",
    "            # Calculate the cost of our output by comparing it to the\n",
    "            # expected output.\n",
    "            cost += cost_func(a2, y)\n",
    "\n",
    "            # Back propagation.\n",
    "            dz2 = a2 - y\n",
    "            dw2 += np.dot(dz2, a1.T)\n",
    "            db2 += dz2\n",
    "            dz1 = np.dot(w2.T, dz2) * sigmoid_gradient(z1)\n",
    "            dw1 += np.dot(dz1, x.T)\n",
    "            db1 += dz1\n",
    "        dw1 /= m\n",
    "        db1 /= m\n",
    "        dw2 /= m\n",
    "        db2 /= m\n",
    "        cost /= m\n",
    "\n",
    "        if check_gradient is not None:\n",
    "            check_gradient(w1, b1, w2, b2, examples, dw1, db1, dw2, db2)\n",
    "        \n",
    "        w1 -= learning_rate * dw1\n",
    "        b1 -= learning_rate * db1\n",
    "        w2 -= learning_rate * dw2\n",
    "        b2 -= learning_rate * db2\n",
    "        if i % 100 == 0 and print_cost:\n",
    "            print(f\"cost at iteration {i}: {cost[0][0]}\")\n",
    "    return (w1, b1, w2, b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a neural net to learn the XNOR operation.\n",
    "\n",
    "This is obviously a stupid use of a neural net, but I wanted a trivial use case to make sure I understood how things work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural net...\n",
      "\n",
      "cost at iteration 0: 0.6960558242178866\n",
      "cost at iteration 100: 0.6929867887355738\n",
      "cost at iteration 200: 0.6922462617773397\n",
      "cost at iteration 300: 0.6865890109283816\n",
      "cost at iteration 400: 0.6374850000789614\n",
      "cost at iteration 500: 0.5063366329094551\n",
      "cost at iteration 600: 0.24494224360510392\n",
      "cost at iteration 700: 0.0915469133772983\n",
      "cost at iteration 800: 0.05089111333463649\n",
      "cost at iteration 900: 0.034477698445246695\n",
      "cost at iteration 1000: 0.025856753007441634\n",
      "cost at iteration 1100: 0.020601465948381907\n",
      "cost at iteration 1200: 0.017082139409163984\n",
      "cost at iteration 1300: 0.014568584508727361\n",
      "cost at iteration 1400: 0.012687400482552087\n",
      "cost at iteration 1500: 0.011228647852513835\n",
      "cost at iteration 1600: 0.010065565651741971\n",
      "cost at iteration 1700: 0.009117240056884511\n",
      "cost at iteration 1800: 0.008329668454476488\n",
      "cost at iteration 1900: 0.007665468228175891\n",
      "cost at iteration 2000: 0.007097960769063973\n",
      "cost at iteration 2100: 0.00660760840370299\n",
      "cost at iteration 2200: 0.006179782389089394\n",
      "cost at iteration 2300: 0.005803316910980904\n",
      "cost at iteration 2400: 0.00546954463742291\n",
      "cost at iteration 2500: 0.005171636891437952\n",
      "cost at iteration 2600: 0.004904141996561449\n",
      "cost at iteration 2700: 0.0046626557805682384\n",
      "cost at iteration 2800: 0.004443582179887452\n",
      "cost at iteration 2900: 0.0042439564984879975\n",
      "cost at iteration 3000: 0.0040613130194073225\n",
      "cost at iteration 3100: 0.0038935845245969417\n",
      "cost at iteration 3200: 0.0037390251105135497\n",
      "cost at iteration 3300: 0.003596150241846593\n",
      "cost at iteration 3400: 0.0034636897193797327\n",
      "cost at iteration 3500: 0.003340550433252222\n",
      "cost at iteration 3600: 0.003225786609225934\n",
      "cost at iteration 3700: 0.0031185758487546865\n",
      "cost at iteration 3800: 0.0030181996897365067\n",
      "cost at iteration 3900: 0.002924027724472616\n",
      "cost at iteration 4000: 0.002835504538852294\n",
      "cost at iteration 4100: 0.0027521389056332163\n",
      "cost at iteration 4200: 0.0026734947912097923\n",
      "cost at iteration 4300: 0.0025991838309167093\n",
      "cost at iteration 4400: 0.0025288590008482473\n",
      "cost at iteration 4500: 0.0024622092702146657\n",
      "cost at iteration 4600: 0.0023989550616518632\n",
      "cost at iteration 4700: 0.0023388443807303714\n",
      "cost at iteration 4800: 0.002281649502465213\n",
      "cost at iteration 4900: 0.002227164123604663\n",
      "\n",
      "Training complete. Verifying predictions...\n",
      "\n",
      "[[ 1.]\n",
      " [ 1.]] should be approximately 1.0...\n",
      "  ✓ Prediction is 0.9979780086180247, hooray!\n",
      "\n",
      "[[ 0.]\n",
      " [ 0.]] should be approximately 1.0...\n",
      "  ✓ Prediction is 0.9972360640361466, hooray!\n",
      "\n",
      "[[ 0.]\n",
      " [ 1.]] should be approximately 0.0...\n",
      "  ✓ Prediction is 0.0019519805833150073, hooray!\n",
      "\n",
      "[[ 1.]\n",
      " [ 0.]] should be approximately 0.0...\n",
      "  ✓ Prediction is 0.001953204399139113, hooray!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training neural net...\\n\")\n",
    "\n",
    "# Reuse our truth table as our training data.\n",
    "w1, b1, w2, b2 = train_nn(xnor_truth_table, 5000, 1)\n",
    "\n",
    "# Test our NN to make sure it produces the same responses as our truth table.\n",
    "# Note that normally a NN classifier would use some sort of thresholding\n",
    "# to determine whether its outputs are true or false, but here we'll just\n",
    "# directly compare its output to the expected truth table value to two\n",
    "# decimal places, because our NN happens to be that awesome.\n",
    "print(f\"\\nTraining complete. Verifying predictions...\\n\")\n",
    "for x, y in xnor_truth_table:\n",
    "    print(f\"{x} should be approximately {float(y)}...\")\n",
    "    y_hat = predict_nn(w1, b1, w2, b2, x)\n",
    "    assert_almost_equal(y_hat, y, decimal=2)\n",
    "    print(f\"  ✓ Prediction is {float(y_hat)}, hooray!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! But how do we know our gradient descent math is correct?\n",
    "\n",
    "We can figure this out with gradient checking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients check out OK!\n"
     ]
    }
   ],
   "source": [
    "def check_gradient(w1, b1, w2, b2, examples, check_dw1, check_db1, check_dw2, check_db2):\n",
    "    epsilon = 0.0001\n",
    "    m = len(examples)\n",
    "\n",
    "    theta = np.concatenate((\n",
    "        w1.reshape(-1, 1),\n",
    "        b1.reshape(-1, 1),\n",
    "        w2.reshape(-1, 1),\n",
    "        b2.reshape(-1, 1),\n",
    "    ))\n",
    "    costs_left = np.zeros([len(theta), 1])\n",
    "    costs_right = np.copy(costs_left)\n",
    "    \n",
    "    def unrolled_predict_nn(theta, x):\n",
    "        w1 = theta[0:4].reshape(2, 2)\n",
    "        b2 = theta[4:6].reshape(2, 1)\n",
    "        w2 = theta[6:8].reshape(1, 2)\n",
    "        b2 = theta[8:9].reshape(1, 1)\n",
    "        return predict_nn(w1, b1, w2, b2, x)\n",
    "\n",
    "    for x, y in examples:\n",
    "        for i in range(len(theta)):\n",
    "            theta_left = np.copy(theta)\n",
    "            theta_left[i] -= epsilon\n",
    "            theta_right = np.copy(theta)\n",
    "            theta_right[i] += epsilon\n",
    "            costs_left[i] += cost_func(unrolled_predict_nn(theta_left, x), y)[0]\n",
    "            costs_right[i] += cost_func(unrolled_predict_nn(theta_right, x), y)[0]\n",
    "\n",
    "    costs_left /= m\n",
    "    costs_right /= m\n",
    "    theta_prime = (costs_right - costs_left) / (2 * epsilon)\n",
    "    dw1 = theta_prime[0:4].reshape(2, 2)\n",
    "    db1 = theta_prime[4:6].reshape(2, 1)\n",
    "    dw2 = theta_prime[6:8].reshape(1, 2)\n",
    "    db2 = theta_prime[8:9].reshape(1, 1)\n",
    "    assert_almost_equal(dw1, check_dw1)\n",
    "    assert_almost_equal(db1, check_db1, decimal=2)  # TODO: Why the disparity here?\n",
    "    assert_almost_equal(dw2, check_dw2)\n",
    "    assert_almost_equal(db2, check_db2)\n",
    "\n",
    "train_nn(xnor_truth_table, 10, 1, check_gradient=check_gradient, print_cost=False)\n",
    "\n",
    "print(\"Gradients check out OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
